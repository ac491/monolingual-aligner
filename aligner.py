# -*- coding: utf-8 -*-
"""NodeAlignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xzz6pSNX03OBx8xyqSqCWGqw2ww_Q0wb

Installing spacy and the required model
"""
import glove
from glove import Glove
import nltk
from nltk.corpus import wordnet as wn
import itertools
import spacy
import math
from gensim.models import FastText
import json



class Alignment:

  def __init__(self, fasttext_model300, glove_model300):
    self.glove_model300 = glove_model300
    self.fasttext_model300 = fasttext_model300
    nltk.download('stopwords')
    nltk.download('punkt')
    nltk.download('wordnet')
    self.ppdbScore = 0.9
    PATH = 'drive/My Drive/ppdb/'
    self.data = {}
    self.THRESHOLD = 0.65
    self.entities = []
    with open(PATH + 'ppdbDict.txt') as json_file:  
      self.data = json.load(json_file)
      
    

  """Function: textContext

  Input: 
    1. S, T: Sentences to be aligned
    2. i: Index of a word in S 
    3. j: Index of a word in T 
    4. stop_words: A set of stop words 

  Output:
    context ={(k,l)}: pairs of word indexes
  """

  #Find the textual context of the pair of words from the two sentences

  def textContext(self, S, T, i, j, stop_words):
    context_range_i = self.getRange(i, len(S))
    C_i = [k for k in context_range_i if k != i and S[k] not in stop_words]
    context_range_j = self.getRange(j, len(T))
    C_j = [l for l in context_range_j if l != j and T[l] not in stop_words]
    
    #find carestian product of C_i and C_j
    context = []
    for element in itertools.product(*[C_i, C_j]):
      context.append(element)
    return context

  # get the context range [i-3, i+3]
  def getRange(self, index, size):
    start = 0
    end = 0
    
    if index<=3:
      start = 0
    else:
      start = index - 3
    
    if (index + 3) >= size:
      end = size - 1
    else:
      end = index + 3
    
    return range(start, end)

  """###Extracting contextual evidence

  Formally, given two input sentences S and T, we consider two words s ∈ S and t ∈ T to form a candidate pair for alignment if∃$r_{s}$ ∈ S and ∃$r_{t} ∈ T$ such that: 
    1. (s,t) ∈ $R_{Sim}$ and ($r_{s},r_{t}$) ∈ $R_{Sim}$, where $R_{Sim}$ is a binary relation indicating sufﬁcient semantic relatedness between the members of each pair (≥ ppdbSim in our case).
    
    2. (s,$r_{s}$) ∈ $R_{C1}$ and (t,$r_{t}$) ∈ $R_{C2}$, such that  $R_{C1}$ ≈ $R_{C2}$; where  $R_{C1}$ and  $R_{C2}$ are binary relations representing speciﬁc types of contextual relationships between two words in a sentence (e.g., an nsubj dependency between a verb and a noun). The symbol≈represents equivalence between two relationships, including identicality.
  """


  def EQ(self, pos_s, pos_rs, s_dep_type, t_dep_type):
  
    if pos_s == 'VERB':
      
      if pos_rs == 'VERB':
        
        if s_dep_type in ['purpcl', 'xcomp'] and t_dep_type in ['purpcl', 'xcomp']:
          return True
        if s_dep_type in ['conj_and']  and t_dep_type in ['conj_and']:
          return True
        if s_dep_type in ['conj_or']  and t_dep_type in ['conj_or']:
          return True
        if s_dep_type in ['conj_nor']  and t_dep_type in ['conj_nor']:
          return True
        
      elif pos_rs == 'NOUN':
        
        if s_dep_type in ['agent', 'nsubj', 'xsubj'] and t_dep_type in ['agent', 'nsubj', 'xsubj']:
          return True
        if s_dep_type in ['dobj', 'nsubjpass', 'rel'] and (t_dep_type in ['dobj', 'nsubjpass', 'rel'] or t_dep_type in ['infmod', 'partmod', 'rcmod']):
          return True
        if s_dep_type in ['tmod', 'prep_in', 'prep_at', 'prep_on']  and t_dep_type in ['tmod', 'prep_in', 'prep_at', 'prep_on']:
          return True
        if s_dep_type in ['iobj', 'prep_to'] and t_dep_type in ['iobj', 'prep_to']:
          return True
        
      elif pos_rs == 'ADJ':
        
        if s_dep_type in ['acomp'] and t_dep_type in ['cop', 'csubj']:
          return True 

    elif pos_s == 'NOUN':
      
      if pos_rs == 'VERB':
        
        if s_dep_type in ['infmod', 'partmod', 'rcmod'] and t_dep_type in ['infmod', 'partmod', 'rcmod']:
          return True 
        
      elif pos_rs == 'NOUN':
        
        if s_dep_type in ['pos', 'nn', 'prep_of', 'prep_in', 'prep_at', 'prep_for'] and t_dep_type in ['pos', 'nn', 'prep_of', 'prep_in', 'prep_at', 'prep_for']:
          return True
        if s_dep_type in ['conj_and']  and t_dep_type in ['conj_and']:
          return True
        if s_dep_type in ['conj_or']  and t_dep_type in ['conj_or']:
          return True
        if s_dep_type in ['conj_nor']  and t_dep_type in ['conj_nor']:
          return True
        
      elif pos_rs == 'ADJ':
        
        if s_dep_type in ['amod', 'rcmod'] and (t_dep_type in ['amod', 'rcmod'] or t_dep_type in ['nsubj']):
          return True
      
    elif pos_s == 'ADJ':
      
      if pos_rs == 'ADJ':
        
        if s_dep_type in ['conj_and']  and t_dep_type in ['conj_and']:
          return True
        if s_dep_type in ['conj_or']  and t_dep_type in ['conj_or']:
          return True
        if s_dep_type in ['conj_nor']  and t_dep_type in ['conj_nor']:
          return True
    
    elif pos_s == 'ADV':
    
      if pos_rs == 'ADJ':
        
        if s_dep_type in ['conj_and']  and t_dep_type in ['conj_and']:
          return True
        if s_dep_type in ['conj_or']  and t_dep_type in ['conj_or']:
          return True
        if s_dep_type in ['conj_nor']  and t_dep_type in ['conj_nor']:
          return True
    
    return False

  """Function: depContext

  Input: 
    1. S, T: Sentences to be aligned 
    2. i: Index of a word in S 
    3. j: Index of a word in T 
    4. EQ: Dependency type equivalences 
    
  Output: 
    context ={(k,l)}: pairs of word indexes
  """

  def depContext(self, S, T, s, t, i, j):
    depGraph_s = self.dependencyGraph(s)
    depGraph_t = self.dependencyGraph(t)
    posDict_s = self.posDict(s)
    posDict_t = self.posDict(t)
    context = []
    for k in range(len(S)):
      for l in range(len(T)):
        if self.findSimilarityFastText(S[k], T[l]) > self.THRESHOLD and self.checkParentChildDep(depGraph_s, S, i, k) and self.checkParentChildDep(depGraph_t, T, j, l) and posDict_s[S[i]] == posDict_t[T[j]] and posDict_s[S[k]] == posDict_t[T[l]] and (self.findParentChildDep(depGraph_s, S, i, k) == self.findParentChildDep(depGraph_t, T, j, l) or self.EQ(posDict_s[S[i]], posDict_s[S[k]], self.findParentChildDep(depGraph_s, S, i, k), self.findParentChildDep(depGraph_t, T, j, l))):
            context.append((k, l))
    return context

  def dependencyGraph(self, S):
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(S)
    graph = {}
    for token in doc:
      loc = graph.setdefault(token.head.text, []) 
      dep = {}
      dep['type'] = token.dep_
      dep['value'] = token.text
      loc.append(dep)
    return graph

  def checkParentChildDep(self, depGraph, S, i, k):
    if S[i] in depGraph:
      for d in depGraph[S[i]]:
        if d['value'] == S[k]:
          return True
    return False

  def posDict(self, S):
    pos_dict = {}
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(S)
    for token in doc:
      pos_dict[token.text] = token.pos_
    return pos_dict

  def findParentChildDep(self, depGraph, S, i, k):
    if S[i] in depGraph:
      for d in depGraph[S[i]]:
        if d['value'] == S[k]:
          return d['type']
    return None

  """Function: wsAlign

  Input:
    1. S, T - The sentences to be aligned
    
  Output: {(k,l)} indeces of aligned words
  """

  def wsAlign(self, S, T):
      m = len(S)
      n = len(T)
      align = []
      L = [[0 for x in range(n+1)] for x in range(m+1)] 
      for i in range(m+1): 
          for j in range(n+1): 
              if i == 0 or j == 0: 
                  L[i][j] = 0
              elif S[i-1] == T[j-1]: 
                  L[i][j] = L[i-1][j-1] + 1
              else: 
                  L[i][j] = max(L[i-1][j], L[i][j-1]) 

      index = L[m][n] 
      i = m 
      j = n 
      while i > 0 and j > 0: 
          if S[i-1] == T[j-1]: 
              align.append((i-1, j-1))
              i-=1
              j-=1
    
          elif L[i-1][j] > L[i][j-1]: 
              i-=1
          else: 
              j-=1
      align.reverse()
      return align

  """Function: neAlign

  Input:
    1. S, T: sentences to be aligned (Tokens)
    2. s, t: sentences to be aligned
    
  Output: {(k, l)} indeces of the alignments
  """

  def neAlign(self, S, T, s, t):
    nlp = spacy.load("en_core_web_sm")
    doc1 = nlp(s)
    doc2 = nlp(t)
    align = []
    
    for ent in doc1.ents:
      if ent.text not in self.entities:
        self.entities.append(ent.text)
        
    for ent in doc2.ents:
      if ent.text not in self.entities:
        self.entities.append(ent.text)
    
    for ent1 in doc1.ents:
      for ent2 in doc2.ents:
        if ent1.text == ent2.text:
          S_index = self.getTokenIndex(S, s, ent1.start_char, ent1.end_char)
          T_index = self.getTokenIndex(T, t, ent2.start_char, ent2.end_char)
          align.append((S_index, T_index))
          break     #to ensure each entity matches to only one other entity
    return align

  def getTokenIndex(self, S, s, s_index, e_index):
    ne = s[s_index:e_index]
    for i, token in enumerate(S):
      if ne == token:
        return i

  def findAllEntities(self, s):
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(s) 
    entity = []
    for ent in doc.ents:
      if ent not in entity:
        entity.append(ent)
    return entity
    
  """Function:  cwDepAlign

  Input: 
    1. S, T: Sentences to be aligned 
    2. EQ: Dependency type equivalences  
    3. A: Already aligned word pair indexes 
    4. w: Weight of word similarity relative to contextual similarity 
    5. STOP: A set of stop words 
    
  Output: A = {(i,j)}: word index pairs of aligned words{(si,tj)}where si ∈ S and tj ∈ T
  """

  def cwDepAlign(self, S, T, s, t, A, w, stop_words):
    
    temp_align = []
    context_set = {}
    weights = []
    
    for i, s_i in enumerate(S):
      for j, t_j in enumerate(T):
        if s_i not in stop_words and not self.checkNumber(s_i) and s_i not in self.entities and s_i not in ['years', 'months', 'year', 'month', 'day', 'days', 'hours', 'hour'] and not [(i,l) for l in range(len(T)) if (i, l) in A] and t_j not in stop_words and not self.checkNumber(t_j) and t_j not in self.entities and t_j not in ['years', 'months', 'year', 'month', 'day', 'days', 'hours', 'hour'] and not [(k,j) for k in range(len(S)) if (k, j) in A] and self.findSimilarityFastText(s_i, t_j) > self.THRESHOLD:
          context = self.depContext(S, T, s, t, i, j)
          contextSim = self.wordSimSum(S, T, context)
          if contextSim > 0:
            temp_align.append((i, j))
            weights.append(w * self.findSimilarityFastText(s_i, t_j) + (1-w) * contextSim)
            context_set[(i, j)] = context
      
    
    temp_align = [x for _,x in sorted(zip(weights, temp_align))]  #sort text_align according to decresing order of weights
    temp_align.reverse()
    align = []
    
    for (i, j) in temp_align:
      if not [(i,l) for l in range(len(T)) if (i, l) in align] and not [(k,j) for k in range(len(S)) if (k, j) in align]:
        if (i, j) not in align:
          align.append((i, j))
      for (k, l) in context_set[(i, j)]:
        if not [(k,q) for q in range(len(T)) if (k, q) in (align + A)] and not [(p,l) for p in range(len(S)) if (p, l) in (align + A)]:
          if (k, l) not in align:
            align.append((k, l))
    
    return align

  def wordSimSum(self, S, T, context):
    s_sum = 0
    for t in context:
      s_sum += self.findSimilarityFastText(S[t[0]], T[t[1]])
    return s_sum

  """Function: cwTextAlign

  Input: 
    1. S, T: Sentences to be aligned 
    2. A: Existing alignments by word indexes 
    3. w: Weight of word similarity relative to contextual similarity 
    4. stop_words: A set of stop words
    
  Output: 
    A = {(i,j)}: word index pairs of aligned words{(si,tj)}where si ∈ S and tj ∈ T
  """

  def cwTextAlign(self, S, T, A, w, stop_words):
    temp_align = []
    weights = []
    
    for i, s_i in enumerate(S):
      for j, t_j in enumerate(T):
        if s_i not in stop_words and not self.checkNumber(s_i) and s_i not in self.entities and s_i not in ['years', 'months', 'year', 'month', 'day', 'days', 'hours', 'hour'] and not [(i,l) for l in range(len(T)) if (i, l) in A] and t_j not in stop_words and not self.checkNumber(t_j) and t_j not in self.entities and t_j not in ['years', 'months', 'year', 'month', 'day', 'days', 'hours', 'hour'] and not [(k,j) for k in range(len(S)) if (k, j) in A] and self.findSimilarityFastText(s_i, t_j) > self.THRESHOLD:
          context = self.textContext(S, T, i, j, stop_words)
          contextSim = self.wordSimSum(S, T, context)
          temp_align.append((i, j))
          weights.append(w * self.findSimilarityFastText(s_i, t_j) + (1-w) * contextSim)
          
    temp_align = [x for _,x in sorted(zip(weights, temp_align))]  #sort text_align according to decresing order of weights
    temp_align.reverse()
    align = []
    
    for (i, j) in temp_align:
      if not [(i,l) for l in range(len(T)) if (i, l) in align] and not [(k,j) for k in range(len(S)) if (k, j) in align]:
        if (i, j) not in align:
          align.append((i, j))
          
    return align

  """For stop words:

    1.  we ignore type equivalences for stop words and implement only exact matching of dependencies
    2. Rather than considering all semantically similar word pairs for contextual similarity, we consider only aligned pairs
  """

  def swDepContext(self, S, T, s, t, i, j):
    depGraph_s = self.dependencyGraph(s)
    depGraph_t = self.dependencyGraph(t)
    posDict_s = self.posDict(s)
    posDict_t = self.posDict(t)
    context = []
    for k in range(len(S)):
      for l in range(len(T)):
        if self.findSimilarityFastText(S[k], T[l]) > self.THRESHOLD and self.checkParentChildDep(depGraph_s, S, i, k) and self.checkParentChildDep(depGraph_t, T, j, l) and posDict_s[S[i]] == posDict_t[T[j]] and posDict_s[S[k]] == posDict_t[T[l]] and self.findParentChildDep(depGraph_s, S, i, k) == self.findParentChildDep(depGraph_t, T, j, l):
            context.append((k, l))
    return context

  """Function: swDepAlign

  Input:

    1. S, T: Sentences to be aligned
    2. A: Already aligned word pair indexes
    3. w: Weight of word similarity relative to contextual similarity
    4. STOP: A set of stop words

  Output:
    A = {(i,j)}: word index pairs of aligned words{(si,tj)}where si ∈ S and tj ∈ T
  """

  def swDepAlign(self, S, T, s, t, A, w, stop_words):
    
    temp_align = []
    context_set = {}
    weights = []
    
  
    for i, s_i in enumerate(S):
      for j, t_j in enumerate(T):
        if s_i in stop_words and not [(i,l) for l in range(len(T)) if (i, l) in A] and t_j in stop_words and not [(k,j) for k in range(len(S)) if (k, j) in A] and self.findSimilarityFastText(s_i, t_j) > self.THRESHOLD:
          context = self.swDepContext(S, T, s, t, i, j)
          context = [c for c in context if c in A]     #instead of considering all semantically similar word pairs for contextual similarity, we consider only aligned pairs
          contextSim = self.wordSimSum(S, T, context)
          if contextSim > 0:
            temp_align.append((i, j))
            weights.append(w * self.findSimilarityFastText(s_i, t_j) + (1-w) * contextSim)
            context_set[(i, j)] = context
      
    
    temp_align = [x for _,x in sorted(zip(weights, temp_align))]  #sort text_align according to decresing order of weights
    temp_align.reverse()
    align = []
    
    for (i, j) in temp_align:
      if not [(i,l) for l in range(len(T)) if (i, l) in align] and not [(k,j) for k in range(len(S)) if (k, j) in align]:
        if (i, j) not in align:
          align.append((i, j))
      for (k, l) in context_set[(i, j)]:
        if not [(k,q) for q in range(len(T)) if (k, q) in (align + A)] and not [(p,l) for p in range(len(S)) if (p, l) in (align + A)]:
          if (k, l) not in align:
            align.append((k, l))
    
    return align

  """Function: swTextAlign

  Input: 
    1. S, T: Sentences to be aligned 
    2. A: Existing alignments by word indexes 
    3. w: Weight of word similarity relative to contextual similarity 
    4. stop_words: A set of stop words
    
  Output: 
    A = {(i,j)}: word index pairs of aligned words{(si,tj)}where si ∈ S and tj ∈ T
  """

  def swTextAlign(self, S, T, A, w, stop_words):
    temp_align = []
    weights = []
    
    for i, s_i in enumerate(S):
      for j, t_j in enumerate(T):
        if s_i in stop_words and not [(i,l) for l in range(len(T)) if (i, l) in A] and t_j in stop_words and not [(k,j) for k in range(len(S)) if (k, j) in A] and self.findSimilarityFastText(s_i, t_j) > self.THRESHOLD:
          context = self.textContext(S, T, i, j, stop_words)
          context = [c for c in context if c in A]
          contextSim = self.wordSimSum(S, T, context)
          temp_align.append((i, j))
          weights.append(w * self.findSimilarityFastText(s_i, t_j) + (1-w) * contextSim)
          
    temp_align = [x for _,x in sorted(zip(weights, temp_align))]  #sort text_align according to decresing order of weights
    temp_align.reverse()
    align = []
    
    for (i, j) in temp_align:
      if not [(i,l) for l in range(len(T)) if (i, l) in align] and not [(k,j) for k in range(len(S)) if (k, j) in align]:
        if (i, j) not in align:
          align.append((i, j))
          
    return align

  """Function: align

  Input: 
    1. S, T: Sentences to be aligned 
    2. EQ: Dependency type equivalences 
    3. w: Weight of word similarity relative to contextual similarity 
    4. stop_words: A set of stop words 
    
  Output: 
    A = {(i,j)}: word index pairs of aligned words{(si,tj)}where si ∈ S and tj ∈ T
  """

  def align(self, S, T, s, t, w, stop_words):
    A = []
    A = self.wsAlign(S, T)
    print(A)
    A = A + self.neAlign(S, T, s, t)
    print(A)
    A = A + self.cwDepAlign(S, T, s, t, A, w, stop_words)
    print(A)
    A = A + self.cwTextAlign(S, T, A, w, stop_words)
    print(A)
    A = A + self.swDepAlign(S, T, s, t, A, w, stop_words)
    print(A)
    A = A + self.swTextAlign(S, T, A, w, stop_words)
    return A


  def getAlignedWords(self, S, T, A):
    words = []
    for (i, j) in A:
      if i != None and j != None:
        words.append((S[i], T[j]))
    return words

  def getAlignmentScore(self, S, T, A):
    score = 0
    for (i, j) in A:
      score += self.findSimilarityFastText(S[i], T[j])
    return score


  """Function to find Cosine Similarity"""

 
  def normalize(self, l):
    norm = [a*a for a in l]
    s = sum(norm)
    s = math.sqrt(s)
    l = [a/s for a in l]
    return l

  def cosineSim(self, l1, l2):
    l1 = self.normalize(l1)
    l2 = self.normalize(l2)
    sim = [a1*a2 for (a1, a2) in zip(l1, l2)]
    return sum(sim)

  """##Similarity using different vector embeddings"""

  def spacySim(self, s, t, w1, w2):
      nlp = spacy.load('en_core_web_md')
      tokens_s = nlp(s)
      tokens_t = nlp(t)
      v1 = []
      v2 = []
      for token in tokens_s:
        if token.text == w1:
          v1 = token.vector
          break
          
      for token in tokens_t:
        if token.text == w2:
          v2 = token.vector
          break
  
      return self.cosineSim(v1, v2)
 
  def checkPPDB(self, w1, w2):
    if w1 in self.data:
      if self.data[w1]['target'] == w2:
        return True
    elif w2 in self.data:
      if self.data[w2]['target'] == w1:
        return True

    return False

  def findSimilarityFastText(self, w1, w2):
    #fasttext_model300 = FastText.load('drive/My Drive/wiki-news-300d-1M-subword.vec')
    if w1 in fasttext_model300.vocab and w2 in fasttext_model300.vocab:
      v1 = self.fasttext_model300[w1]
      v2 = self.fasttext_model300[w2]
      return self.cosineSim(v1, v2)
    elif self.checkPPDB(w1, w2):
      return self.ppdbScore
    else:
      return 0

  def findSimilarityGlove(self, w1, w2):
    v1 = self.glove_model300[w1]
    v2 = self.glove_model300[w2]
    return self.cosineSim(v1, v2)
  
  def checkNumber(self, s):
    try:
        float(s)
        return True
    except ValueError:
        pass
 
    try:
        import unicodedata
        unicodedata.numeric(s)
        return True
    except (TypeError, ValueError):
        pass
 
    return False
